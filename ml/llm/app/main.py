from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from contextlib import asynccontextmanager
import asyncio
import json
import logging
import os
import time
from typing import Dict

from .process_manager import ProcessManager
from .schemas import ChatCompletionRequest, CompletionRequest, ModelListResponse

# üî¥ –í–ê–ñ–ù–û: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –≤—ã–≤–æ–¥–æ–º –≤ –∫–æ–Ω—Å–æ–ª—å
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
process_manager = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    global process_manager
    
    # –ë–µ—Ä–µ–º –ø—É—Ç–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ
    llama_cpp_path = os.getenv("LLAMA_CPP_PATH", "./llama-server")
    models_dir = os.getenv("MODELS_DIR", "./models")
    
    # –ï—Å–ª–∏ –ø—É—Ç–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ, –¥–µ–ª–∞–µ–º –∏—Ö –∞–±—Å–æ–ª—é—Ç–Ω—ã–º–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–±–æ—á–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    if not os.path.isabs(llama_cpp_path):
        llama_cpp_path = os.path.join(os.getcwd(), llama_cpp_path)
    if not os.path.isabs(models_dir):
        models_dir = os.path.join(os.getcwd(), models_dir)
    
    logger.info(f"üöÄ Llama.cpp path: {llama_cpp_path}")
    logger.info(f"üìö Models dir: {models_dir}")
    logger.info(f"üìÅ Current working directory: {os.getcwd()}")
    
    process_manager = ProcessManager(
        llama_cpp_path=llama_cpp_path,
        models_dir=models_dir,
        inactivity_timeout=300
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
    cleanup_task = asyncio.create_task(cleanup_inactive_servers())
    
    yield
    
    # Shutdown
    cleanup_task.cancel()
    try:
        await cleanup_task
    except asyncio.CancelledError:
        pass
    
    if process_manager:
        await process_manager.cleanup_all()

app = FastAPI(
    title="Llama.cpp OpenAI API",
    description="OpenAI-compatible API for llama.cpp with on-demand server startup",
    version="1.0.0",
    lifespan=lifespan
)

async def cleanup_inactive_servers():
    """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤"""
    while True:
        try:
            if process_manager:
                await process_manager.cleanup_inactive()
        except Exception as e:
            logger.error(f"Error in cleanup task: {e}", exc_info=True)
        await asyncio.sleep(5)

@app.get("/v1/models")
async def list_models():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    if not process_manager:
        raise HTTPException(status_code=500, detail="Process manager not initialized")
    
    models = process_manager.get_available_models()
    logger.info(f"üìã Available models: {models}")
    return ModelListResponse(data=[
        {
            "id": model_name,
            "object": "model",
            "owned_by": "local"
        }
        for model_name in models
    ])

@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest):
    """Chat completion endpoint (OpenAI compatible)"""
    logger.debug(f"üí¨ Received chat completion request: model={request.model}, messages={len(request.messages)}")
    
    if not process_manager:
        raise HTTPException(status_code=500, detail="Process manager not initialized")
    
    try:
        # üî¥ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–ª—É—á–∞–µ–º URL –∑–∞–ø—É—â–µ–Ω–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞
        logger.info(f"üîÑ Getting server for model: {request.model}")
        base_url = await process_manager.get_server_for_model(request.model)
        logger.info(f"‚úÖ Server URL obtained: {base_url}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è llama.cpp –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞
        prompt = ""
        for msg in request.messages:
            role = msg.role
            content = msg.content
            
            if role == "system":
                prompt += f"### System:\n{content}\n\n"
            elif role == "user":
                prompt += f"### User:\n{content}\n\n"
            elif role == "assistant":
                prompt += f"### Assistant:\n{content}\n\n"
        
        prompt += "### Assistant:\n"
        
        logger.debug(f"üìù Generated prompt (first 200 chars): {prompt[:200]}...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è llama.cpp
        params = {
            "prompt": prompt,
            "stream": request.stream,
            "n_predict": request.max_tokens or 512,
            "temperature": request.temperature or 0.7,
            "top_p": request.top_p or 0.95,
            "stop": request.stop or ["### User:"],
            "repeat_penalty": 1.1,
            "top_k": 40
        }
        
        logger.debug(f"‚öôÔ∏è Request params: {params}")
        
        if request.stream:
            logger.info("üåä Streaming response")
            return StreamingResponse(
                stream_completion(base_url, params, request.model),
                media_type="text/event-stream"
            )
        else:
            import httpx
            logger.info(f"üì° Sending request to {base_url}/completion")
            
            async with httpx.AsyncClient(timeout=300.0) as client:
                response = await client.post(
                    f"{base_url}/completion",
                    json=params,
                    headers={"Content-Type": "application/json"}
                )
                
                logger.info(f"üì® Response status: {response.status_code}")
                logger.debug(f"üì® Response body: {response.text[:500]}")
                
                if response.status_code != 200:
                    error_detail = response.text
                    logger.error(f"‚ùå llama.cpp server error: {error_detail}")
                    raise HTTPException(
                        status_code=response.status_code,
                        detail=error_detail
                    )
                
                result = response.json()
                logger.debug(f"‚úÖ Got result from llama.cpp: {result}")
                
                await process_manager.update_activity(request.model)
                
                response_data = {
                    "id": f"chatcmpl-{hash(prompt)}",
                    "object": "chat.completion",
                    "created": int(time.time()),
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": result.get("content", "")
                        },
                        "finish_reason": "stop"
                    }],
                    "usage": {
                        "prompt_tokens": result.get("tokens_evaluated", 0),
                        "completion_tokens": result.get("tokens_predicted", 0),
                        "total_tokens": result.get("tokens_evaluated", 0) + result.get("tokens_predicted", 0)
                    }
                }
                
                logger.info(f"‚úÖ Returning chat completion response")
                return response_data
            
    except ValueError as e:
        logger.error(f"‚ùå ValueError: {e}")
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"‚ùå Error in chat completion: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/completions")
async def create_completion(request: CompletionRequest):
    """Text completion endpoint"""
    logger.debug(f"üìù Received completion request: model={request.model}")
    
    if not process_manager:
        raise HTTPException(status_code=500, detail="Process manager not initialized")
    
    try:
        # üî¥ –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ë—ã–ª–æ update_activity –≤–º–µ—Å—Ç–æ get_server_for_model!
        logger.info(f"üîÑ Getting server for model: {request.model}")
        base_url = await process_manager.get_server_for_model(request.model)  # ‚Üê –ò–°–ü–†–ê–í–õ–ï–ù–û!
        logger.info(f"‚úÖ Server URL obtained: {base_url}")
        
        params = {
            "prompt": request.prompt if isinstance(request.prompt, str) else "\n".join(request.prompt),
            "stream": request.stream,
            "n_predict": request.max_tokens or 512,
            "temperature": request.temperature or 0.7,
            "top_p": request.top_p or 0.95,
            "stop": request.stop,
            "repeat_penalty": 1.1
        }
        
        logger.debug(f"‚öôÔ∏è Request params: {params}")
        
        if request.stream:
            logger.info("üåä Streaming response")
            return StreamingResponse(
                stream_completion(base_url, params, request.model),
                media_type="text/event-stream"
            )
        else:
            import httpx
            logger.info(f"üì° Sending request to {base_url}/completion")
            
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    f"{base_url}/completion",
                    json=params,
                    headers={"Content-Type": "application/json"}
                )
                
                logger.info(f"üì® Response status: {response.status_code}")
                logger.debug(f"üì® Response body: {response.text[:500]}")
                
                if response.status_code != 200:
                    error_detail = response.text
                    logger.error(f"‚ùå llama.cpp server error: {error_detail}")
                    raise HTTPException(
                        status_code=response.status_code,
                        detail=error_detail
                    )
                
                result = response.json()
                logger.debug(f"‚úÖ Got result from llama.cpp: {result}")
                
                await process_manager.update_activity(request.model)
                
                response_data = {
                    "id": f"cmpl-{hash(str(request.prompt))}",
                    "object": "text_completion",
                    "created": int(time.time()),
                    "model": request.model,
                    "choices": [{
                        "text": result.get("content", ""),
                        "index": 0,
                        "finish_reason": "stop"
                    }],
                    "usage": {
                        "prompt_tokens": result.get("tokens_evaluated", 0),
                        "completion_tokens": result.get("tokens_predicted", 0),
                        "total_tokens": result.get("tokens_evaluated", 0) + result.get("tokens_predicted", 0)
                    }
                }
                
                logger.info(f"‚úÖ Returning completion response")
                return response_data
            
    except ValueError as e:
        logger.error(f"‚ùå ValueError: {e}")
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"‚ùå Error in completion: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

async def stream_completion(base_url: str, params: Dict, model_name: str):
    """Stream –æ—Ç–≤–µ—Ç"""
    import httpx
    import time
    
    logger.info(f"üåä Starting stream to {base_url}/completion")
    
    async with httpx.AsyncClient(timeout=120.0) as client:
        async with client.stream(
            "POST",
            f"{base_url}/completion",
            json={**params, "stream": True},
            headers={"Content-Type": "application/json"}
        ) as response:
            
            request_id = f"chatcmpl-{int(time.time())}"
            logger.debug(f"üåä Stream request ID: {request_id}")
            
            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    data = line[6:]
                    
                    # [DONE] marker
                    if data.strip() == "[DONE]":
                        logger.debug("üåä Stream completed")
                        yield "data: [DONE]\n\n"
                        break
                    
                    try:
                        json_data = json.loads(data)
                        content = json_data.get("content", "")
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º —á–∞–Ω–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI
                        chunk = {
                            "id": request_id,
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": model_name,
                            "choices": [{
                                "index": 0,
                                "delta": {"content": content} if content else {},
                                "finish_reason": None
                            }]
                        }
                        
                        # –í–ê–ñ–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º ensure_ascii=False –¥–ª—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤!
                        yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                        
                    except json.JSONDecodeError as e:
                        logger.warning(f"‚ö†Ô∏è Failed to parse line: {line} | Error: {e}")
                        continue
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ü–û–°–õ–ï –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å—Ç—Ä–∏–º–∞
            if process_manager:
                await process_manager.update_activity(model_name)
                logger.debug(f"‚úÖ Updated activity for model: {model_name}")

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    if process_manager:
        return {
            "status": "healthy", 
            "models_available": len(process_manager.get_available_models()),
            "active_servers": len(process_manager.active_servers)
        }
    return {"status": "initializing"}

if __name__ == "__main__":
    import uvicorn
    logger.info("üöÄ Starting server on http://0.0.0.0:8080")
    uvicorn.run(app, host="0.0.0.0", port=8080, log_level="debug")